{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43c5ca6a-125a-4118-b3d2-1e45626d1212",
   "metadata": {},
   "source": [
    "# Current Trends in Bioinformatics Software Development and Archiving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2092ef92-81df-4128-a1fa-f728c558bac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "\n",
    "import dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import warnings\n",
    "import xmltodict\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import pbmd_tools as pbmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1e66c109-89bd-4f6f-8d99-89ed18a4be3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbmd.read_tokens()\n",
    "GITHUB_TOKEN = os.environ.get(\"GITHUB_TOKEN\")\n",
    "PUBMED_TOKEN = os.environ.get(\"PUBMED_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0068b37-9ae4-444b-874d-2cd5e173b7e5",
   "metadata": {},
   "source": [
    "## 1. PubMed API Entrez Interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81134853-e4da-4e60-aff1-f34443f4ae7b",
   "metadata": {},
   "source": [
    "First of all we are going to explore PubMed in order to find out how many publications for each among 5 forges are there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "188978bd-1f8f-4fc5-9928-9157a341cda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = \"pubmed\"\n",
    "domain = \"https://www.ncbi.nlm.nih.gov/entrez/eutils\"\n",
    "retmode = \"json\"\n",
    "queries_github = []\n",
    "queries_gitlab = []\n",
    "queries_sourceforge = []\n",
    "queries_googlecode = []\n",
    "queries_bitbucket = []\n",
    "\n",
    "#creating queries for every forge and every year\n",
    "for year in range(2009, 2023):\n",
    "    queries_github.append(f'((github.com[Title/Abstract])) AND ((\"{year}/01/01\"[Date - Publication] : \"{year}/12/31\"[Date - Publication]))')\n",
    "    queries_gitlab.append(f'((https://gitlab[Title/Abstract])) AND ((\"{year}/01/01\"[Date - Publication] : \"{year}/12/31\"[Date - Publication]))')\n",
    "    queries_sourceforge.append(f'((sourceforge.net[Title/Abstract])) AND ((\"{year}/01/01\"[Date - Publication] : \"{year}/12/31\"[Date - Publication]))')\n",
    "    queries_googlecode.append(f'(googlecode) AND (\"{year}/01/01\"[Date - Publication] : \"{year}/12/31\"[Date - Publication])')\n",
    "    queries_bitbucket.append(f'(bitbucket.org[Title/Abstract]) AND (\"{year}/01/01\"[Date - Publication] : \"{year}/12/31\"[Date - Publication])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "975bb2c5-a417-48a3-adcd-198c2cb08332",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 14/14 [00:11<00:00,  1.22it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 14/14 [00:09<00:00,  1.46it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 14/14 [00:08<00:00,  1.66it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 14/14 [00:08<00:00,  1.59it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 14/14 [00:09<00:00,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10882 articles with 'github.com' found in PubMed\n",
      "\n",
      "10882 unique articles with 'github.com' found in PubMed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#dictionaries for stocking the number of articles for each forge for each year\n",
    "#example: {'2009': 0, '2010': 5, '2011': 15, ... }\n",
    "\n",
    "stats_github = {}\n",
    "stats_gitlab = {}\n",
    "stats_sourceforge = {}\n",
    "stats_googlecode = {}\n",
    "stats_bitbucket = {}\n",
    "PMIDs = []\n",
    "PMIDs_all = []\n",
    "\n",
    "for query in tqdm(queries_github):\n",
    "    nb = 0 #number of articles for this query\n",
    "    queryLinkSearch = f\"{domain}/esearch.fcgi?db={db}&retmode={retmode}&retmax=15000&term={query}\"\n",
    "    response = requests.get(queryLinkSearch)\n",
    "    pubmed_json = response.json()\n",
    "    for id in pubmed_json[\"esearchresult\"][\"idlist\"]:\n",
    "        #checking if there are any dublicates in PubMed IDs (it happens because of the PubDate that can be EPubDate or normal)\n",
    "        if id not in PMIDs:\n",
    "            nb += 1\n",
    "            PMIDs.append(id)\n",
    "    #query[38:42] - it is the year of this query\n",
    "    stats_github[query[38:42]] = nb \n",
    "    \n",
    "for query in tqdm(queries_bitbucket):\n",
    "    nb = 0\n",
    "    queryLinkSearch = f\"{domain}/esearch.fcgi?db={db}&retmode={retmode}&retmax=15000&term={query}\"\n",
    "    response = requests.get(queryLinkSearch)\n",
    "    pubmed_json = response.json()\n",
    "    for id in pubmed_json[\"esearchresult\"][\"idlist\"]:\n",
    "        if id not in PMIDs_all:\n",
    "            nb += 1\n",
    "            PMIDs_all.append(id)\n",
    "    stats_bitbucket[query[38:42]] = nb\n",
    "    \n",
    "for query in tqdm(queries_gitlab):\n",
    "    nb = 0\n",
    "    queryLinkSearch = f\"{domain}/esearch.fcgi?db={db}&retmode={retmode}&retmax=15000&term={query}\"\n",
    "    response = requests.get(queryLinkSearch)\n",
    "    pubmed_json = response.json()\n",
    "    for id in pubmed_json[\"esearchresult\"][\"idlist\"]:\n",
    "        if id not in PMIDs_all:\n",
    "            nb += 1\n",
    "            PMIDs_all.append(id)\n",
    "    stats_gitlab[query[42:46]] = nb\n",
    "    \n",
    "for query in tqdm(queries_sourceforge):\n",
    "    nb = 0\n",
    "    queryLinkSearch = f\"{domain}/esearch.fcgi?db={db}&retmode={retmode}&retmax=15000&term={query}\"\n",
    "    response = requests.get(queryLinkSearch)\n",
    "    pubmed_json = response.json()\n",
    "    for id in pubmed_json[\"esearchresult\"][\"idlist\"]:\n",
    "        if id not in PMIDs_all:\n",
    "            nb += 1\n",
    "            PMIDs_all.append(id)\n",
    "    stats_sourceforge[query[43:47]] = nb\n",
    "    \n",
    "for query in tqdm(queries_googlecode):\n",
    "    nb = 0\n",
    "    queryLinkSearch = f\"{domain}/esearch.fcgi?db={db}&retmode={retmode}&retmax=15000&term={query}\"\n",
    "    response = requests.get(queryLinkSearch)\n",
    "    pubmed_json = response.json()\n",
    "    for id in pubmed_json[\"esearchresult\"][\"idlist\"]:\n",
    "        if id not in PMIDs_all:\n",
    "            nb += 1\n",
    "            PMIDs_all.append(id)\n",
    "    stats_googlecode[query[19:23]] = nb\n",
    "\n",
    "print(f\"\\n{len(PMIDs)} articles with 'github.com' found in PubMed\")\n",
    "\n",
    "#checking that there is no duplicates\n",
    "PMIDs = list(set(PMIDs))\n",
    "print(f\"\\n{len(PMIDs)} unique articles with 'github.com' found in PubMed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e968d76d-0ce9-460f-80c4-1803faebded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the statistics to reuse it in another notebook\n",
    "\n",
    "with open(\"PMIDs.txt\", \"w\") as f:\n",
    "    for PMID in PMIDs:\n",
    "        f.write(str(PMID)+\"\\n\")\n",
    "with open(\"stats_github.json\", \"w\") as f:\n",
    "    json.dump(stats_github, f)\n",
    "with open(\"stats_gitlab.json\", \"w\") as f:\n",
    "    json.dump(stats_gitlab, f)\n",
    "with open(\"stats_sourceforge.json\", \"w\") as f:\n",
    "    json.dump(stats_sourceforge, f)    \n",
    "with open(\"stats_googlecode.json\", \"w\") as f:\n",
    "    json.dump(stats_googlecode, f)\n",
    "with open(\"stats_bitbucket.json\", \"w\") as f:\n",
    "    json.dump(stats_bitbucket, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "98176cc7-615e-4060-84e2-f2b2dd617b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pubmed.ncbi.nlm.nih.gov/26262258/ - No DOI in PubMed although there is one in the full text of the article (not from ArXiv), and there are a lot of them\n",
    "# https://pubmed.ncbi.nlm.nih.gov/28269829/ - they give a wrong link https://github.com/SBU-BMI/imageboxs://github.com/SBU-BMI/imagebox but if you use this link :\n",
    "# https://github.com/SBU-BMI/imagebox it works. Yet, i am not sure that it is actually what we are looking for since they also provide another link to github.io \n",
    "# (also incorect) and i think it's more likely that their code is there\n",
    "# PMID = 36789260 - 2 links\n",
    "# https://github.com/tyqGitHub/TYQ/tree/master/GACNNMDA - ????\n",
    "# https://github.com/mofradlab - ?????? (PMID 36786404)\n",
    "# PMID = 26124555 - a space in the link\n",
    "# PMID = 24324759, 22151646 - no space after link\n",
    "# PMID = 23849037 - why + in the end ?\n",
    "# PMID = 36315552 - super smart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4aac81-c8b6-4712-98c2-f0a3e9ff1563",
   "metadata": {},
   "source": [
    "Next we will use API PubMed to gather the information about each article such as the publication date, the doi, the abstract, the title of the article and the journal. We will then analyse this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e8425aae-1da2-443d-a325-dc4a1fbe4814",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b5b5aead-fa36-41c6-af77-8af8138b0ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [01:40<00:00,  1.70s/it]\n"
     ]
    }
   ],
   "source": [
    "#API Pubmed rate limit is 10 request per second with a token and 3 request par second without it\n",
    "\n",
    "#count = 0\n",
    "for PMID in tqdm(pm):\n",
    "    #count += 1\n",
    "    #if count % 10 == 0:\n",
    "    #    time.sleep(1)\n",
    "    \n",
    "    try:\n",
    "        summary = pbmd.get_summary(PMID, PUBMED_TOKEN, \"status.txt\")\n",
    "        abstract = pbmd.get_abstract_from_summary(summary, \"status.txt\")\n",
    "        pubdate = pbmd.get_pubdate_from_summary(summary, \"status.txt\")\n",
    "        title = pbmd.get_title_from_summary(summary, \"status.txt\")\n",
    "        journal = pbmd.get_journal_from_summary(summary, \"status.txt\")\n",
    "        doi = pbmd.get_doi_from_summary(summary, \"status.txt\")\n",
    "    except:\n",
    "        try:\n",
    "            summary = pbmd.get_summary(PMID, PUBMED_TOKEN, \"status.txt\")\n",
    "            abstract = pbmd.get_abstract_from_summary(summary, \"status.txt\")\n",
    "            pubdate = pbmd.get_pubdate_from_summary(summary, \"status.txt\")\n",
    "            title = pbmd.get_title_from_summary(summary, \"status.txt\")\n",
    "            journal = pbmd.get_journal_from_summary(summary, \"status.txt\")\n",
    "            doi = pbmd.get_doi_from_summary(summary, \"status.txt\")\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "    #checking in case the API is bugging \n",
    "    if (pubdate, doi) == (None, None):\n",
    "        time.sleep(2)\n",
    "        summary = pbmd.get_summary(PMID, PUBMED_TOKEN, \"status.txt\")\n",
    "        abstract = pbmd.get_abstract_from_summary(summary, \"status.txt\")\n",
    "        pubdate = pbmd.get_pubdate_from_summary(summary, \"status.txt\")\n",
    "        title = pbmd.get_title_from_summary(summary, \"status.txt\")\n",
    "        journal = pbmd.get_journal_from_summary(summary, \"status.txt\")\n",
    "        doi = pbmd.get_doi_from_summary(summary, \"status.txt\")     \n",
    "\n",
    "    results.append((PMID, pubdate, doi, journal, title, abstract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e11bc34d-7377-4167-8d80-48952e78323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(results)\n",
    "df = df.rename(columns = {0: 'PMID', 1: 'PubDate', 3: 'DOI', 4: 'Journal', 5: 'Title', 6: 'Abstract'})\n",
    "df = df.drop_duplicates(subset = 'PMID')\n",
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8a3bcdb5-1810-4b53-b6e9-f55e7b4c3d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10880"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9a6c9a6e-9a56-4117-b4c9-28bec1d25313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records without publication date is: 59\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of records without publication date is: {len(df[df['PubDate'].isnull()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bbf1270-ff48-48bb-b910-dcdbca765190",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df.columns[-2:], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94f8544e-f411-40c6-bd35-318d762992e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('articles.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d566e858-dc91-44d1-9e85-13a0fc89f892",
   "metadata": {},
   "source": [
    "## 2. Geting links from the obtained data using regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5f1cfe1c-42eb-4958-a13c-8c7d43136ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('articles.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b8ce85de-20f7-4211-830b-4c6e01e0eb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GitHub_link_raw'] = df['Abstract'].astype(str).apply(pbmd.get_link_from_abstract)\n",
    "df['GitHub_link_clean'] = df['GitHub_link_raw'].astype(str).apply(pbmd.clean_link)\n",
    "df['GitHub_owner'] = df['GitHub_link_clean'].apply(pbmd.get_owner_from_link)\n",
    "df['GitHub_repo'] = df['GitHub_link_clean'].apply(pbmd.get_repo_from_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "964e174c-558c-445e-abec-a0b0c2c16b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records with weird abstracts leading to inability to extract a link: 6\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of records with weird abstracts leading to inability to extract a link: {len(df[df['GitHub_owner'].isna()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1d2ae2ce-d877-42af-84d7-96db53d9dae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records without a repository name: 251\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of records without a repository name: {len(df[df['GitHub_repo'].isna()])-len(df[df['GitHub_owner'].isna()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "84639824-a280-494b-b8e8-d2a5b6981006",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('articles.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d124283b-c8d7-4c6a-913f-f64f2390d66a",
   "metadata": {},
   "source": [
    "## 3. GitHub API Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "160e77c7-4980-4dbc-bc92-ff9319cd9480",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('articles.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f6bfbf59-53a2-4a9e-93cd-f509588406a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10623"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PMIDs = df['PMID'][df['GitHub_repo'].notna()].to_list()\n",
    "len(PMIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "36dd1992-56f7-42a8-95ae-810217f93e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 905/905 [05:39<00:00,  2.67it/s]\n"
     ]
    }
   ],
   "source": [
    "for PMID in tqdm(PMIDs[9718:]):\n",
    "\n",
    "    with open(\"gitstat.txt\", \"a\") as f:\n",
    "        f.write(f\"\\n\\n PMID: {PMID}, GitHub link: {df[df['PMID'] == PMID]['GitHub_link_clean'].values[0]}\")\n",
    "\n",
    "    info = pbmd.get_repo_info(df[df['PMID']==PMID]['GitHub_owner'].values[0], df[df['PMID']==PMID]['GitHub_repo'].values[0], GITHUB_TOKEN, \"gitstat.txt\")\n",
    "\n",
    "    if info[\"status\"]: \n",
    "        idx = df.index[df['PMID'] == PMID][0]\n",
    "\n",
    "        df.loc[idx, \"Repo_created_at\"] = pbmd.get_repo_date_created(info)\n",
    "        df.loc[idx, \"Repo_updated_at\"] = pbmd.get_repo_date_updated(info)\n",
    "        df.loc[idx, \"Fork\"] = pbmd.is_fork(info)\n",
    "    else:\n",
    "        \n",
    "        time.sleep(3600)\n",
    "        \n",
    "        info = pbmd.get_repo_info(df[df['PMID']==PMID]['GitHub_owner'].values[0], df[df['PMID']==PMID]['GitHub_repo'].values[0], GITHUB_TOKEN, \"gitstat.txt\")\n",
    "        idx = df.index[df['PMID'] == PMID][0]\n",
    "        df.loc[idx, \"Repo_created_at\"] = pbmd.get_repo_date_created(info)\n",
    "        df.loc[idx, \"Repo_updated_at\"] = pbmd.get_repo_date_updated(info)\n",
    "        df.loc[idx, \"Fork\"] = pbmd.is_fork(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682e0960-eefb-4676-a049-e22cca92418f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "758"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PMIDs = df['PMID'][df['Repo_created_at'].isna()].to_list()\n",
    "len(PMIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fe532216-f508-4ff9-a16e-3bb957b1e344",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('articles.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad760f81-0cf8-4b5d-9015-bda6ac2d2487",
   "metadata": {},
   "source": [
    "## 4. Software Heritage API interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aa51fd7-bd8c-472f-b659-edb3ded49461",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('articles.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca4481bb-3767-415c-bc0d-d460cdc7c5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10847"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PMIDs = df['PMID'][df['GitHub_owner'].notna()].to_list()\n",
    "len(PMIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d60a2e7-1d01-4b23-8500-221e4a29c748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10847/10847 [19:47<00:00,  9.13it/s]  \n"
     ]
    }
   ],
   "source": [
    "for PMID in tqdm(PMIDs[:]):\n",
    "    \n",
    "    try:\n",
    "        info = pbmd.check_is_in_softwh(df[df['PMID']==PMID]['GitHub_link_clean'].values[0])\n",
    "\n",
    "        idx = df.index[df['PMID'] == PMID][0]\n",
    "\n",
    "        df.loc[idx, \"In_SoftWH\"] = pbmd.is_in_softwh(info)\n",
    "        df.loc[idx, \"Archived\"] = pbmd.get_date_archived(info)\n",
    "    except:\n",
    "        try:\n",
    "            info = pbmd.check_is_in_softwh(df[df['PMID']==PMID]['GitHub_link_clean'].values[0])\n",
    "\n",
    "            idx = df.index[df['PMID'] == PMID][0]\n",
    "\n",
    "            df.loc[idx, \"In_SoftWH\"] = pbmd.is_in_softwh(info)\n",
    "            df.loc[idx, \"Archived\"] = pbmd.get_date_archived(info)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3954853c-f227-4a5d-8c24-99694669cdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('articles.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d478c7b-9a74-42fb-b71d-1c594eefdadc",
   "metadata": {},
   "source": [
    "## Unresolved links analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b597f0b6-78f4-4736-9a89-ac0191b0c702",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('no_info2.tsv', sep='\\t',usecols=['PMID', 'PubDate', 'DOI', 'Journal', 'Title', 'Abstract', 'Issue', 'GitHub_link_clean','Correct_link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "6b70ec1b-96dd-43d1-9876-4dac90c542a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles with a wrong link (either a space in the link, or no space after link, etc) : 58\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of articles with a wrong link (either a space in the link, or no space after link, etc) : {len(df1[df1['Issue'] == 'wrong link'])}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "b34f0e2f-1e42-46ff-ae71-f984e2a433a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles with a renamed repository : 11\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of articles with a renamed repository : {len(df1[df1['Issue'] == 'renamed'])}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "a8db6bd5-d473-4a5f-85ab-e8b179186b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles with a deleted repository : 146\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of articles with a deleted repository : {len(df1[df1['Issue'] == 'owner deleted']) + len(df1[df1['Issue'] == 'repo deleted'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fd77b8-8cac-45be-af2f-928dd2d35cc5",
   "metadata": {},
   "source": [
    "Resolving unresolved but existing links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "3db1fcb0-8066-4c19-a853-34ad54833439",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.loc[df['Correct_link'].notna(),'GitHub_owner'] = df1.loc[df['Correct_link'].notna(),'Correct_link'].apply(pbmd.get_owner_from_link)\n",
    "df1.loc[df['Correct_link'].notna(),'GitHub_repo'] = df1.loc[df['Correct_link'].notna(),'Correct_link'].apply(pbmd.get_repo_from_link)\n",
    "\n",
    "df1.loc[df['Correct_link'].isna(),'GitHub_owner'] = df1.loc[df['Correct_link'].isna(),'GitHub_link_clean'].apply(pbmd.get_owner_from_link)\n",
    "df1.loc[df['Correct_link'].isna(),'GitHub_repo'] = df1.loc[df['Correct_link'].isna(),'GitHub_link_clean'].apply(pbmd.get_repo_from_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "35967c10-c389-4542-9dfb-cca1c8a61306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PMIDs = df1['PMID'][df1['GitHub_repo'].notna()].to_list()\n",
    "len(PMIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "2e7d124d-9a2b-443b-8c63-7ee3cbecc7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 215/215 [01:05<00:00,  3.30it/s]\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for PMID in tqdm(PMIDs):\n",
    "    count += 1\n",
    "    if count % 5000 == 0:\n",
    "        time.sleep(3600)\n",
    "    \n",
    "    with open(\"gitstat.txt\", \"a\") as f:\n",
    "        f.write(f\"\\n\\n PMID: {PMID}, GitHub link: {df1[df1['PMID'] == PMID]['Correct_link'].values[0]}\")\n",
    "\n",
    "    info = pbmd.get_repo_info(df1[df1['PMID']==PMID]['GitHub_owner'].values[0], df1[df1['PMID']==PMID]['GitHub_repo'].values[0], GITHUB_TOKEN, \"gitstat.txt\")\n",
    "\n",
    "    idx = df1.index[df['PMID'] == PMID][0]\n",
    "\n",
    "    df1.loc[idx, \"Repo_created_at\"] = pbmd.get_repo_date_created(info)\n",
    "    df1.loc[idx, \"Repo_updated_at\"] = pbmd.get_repo_date_updated(info)\n",
    "    df1.loc[idx, \"Fork\"] = pbmd.is_fork(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "ae1722cd-c089-456a-97b4-5267d944a463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PMIDs = df1['PMID'][df1['Correct_link'].notna()].to_list()\n",
    "len(PMIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "3ea729a9-2a5a-4d81-99c8-7e2718000724",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 69/69 [00:13<00:00,  5.03it/s]\n"
     ]
    }
   ],
   "source": [
    "for PMID in tqdm(PMIDs):\n",
    "    \n",
    "    info = pbmd.check_is_in_softwh(df1[df1['PMID']==PMID]['Correct_link'].values[0])\n",
    "\n",
    "    idx = df1.index[df1['PMID'] == PMID][0]\n",
    "    \n",
    "    df1.loc[idx, \"In_SoftWH\"] = pbmd.is_in_softwh(info)\n",
    "    df1.loc[idx, \"Archived\"] = pbmd.get_date_archived(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "d3c3010b-1b18-4875-bf57-c3e100883ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PMIDs = df1['PMID'][df1['Correct_link'].isna()].to_list()\n",
    "len(PMIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "0d615f28-b18d-4cd0-9407-5d6e88b9e7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 146/146 [00:56<00:00,  2.60it/s]\n"
     ]
    }
   ],
   "source": [
    "for PMID in tqdm(PMIDs):\n",
    "    \n",
    "    info = pbmd.check_is_in_softwh(df1[df1['PMID']==PMID]['GitHub_link_clean'].values[0])\n",
    "\n",
    "    idx = df1.index[df1['PMID'] == PMID][0]\n",
    "    \n",
    "    df1.loc[idx, \"In_SoftWH\"] = pbmd.is_in_softwh(info)\n",
    "    df1.loc[idx, \"Archived\"] = pbmd.get_date_archived(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "414b497b-bf2c-432e-86c7-cfe37b3af1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repositories that are forks : 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Repositories that are forks : {len(df1[df1['Fork'] == 1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "2433e02b-40b0-4b61-8636-a35d3954f887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in SoftWH : 118\n"
     ]
    }
   ],
   "source": [
    "print(f\"Not in SoftWH : {len(df1[df1['In_SoftWH'] == 0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "b561b266-3336-4238-8827-87c3bef2740d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In SoftWH : 97\n"
     ]
    }
   ],
   "source": [
    "print(f\"In SoftWH : {len(df1[df1['In_SoftWH'] == 1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "23fad432-e0f1-4cb4-991b-a88ca809422d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Were deleted but archived in SoftWH : 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nadia\\AppData\\Local\\Temp\\ipykernel_9668\\3996960915.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  print(f\"Were deleted but archived in SoftWH : {len(df[df['Correct_link'].isna()][df['In_SoftWH'] == 1])}\")\n"
     ]
    }
   ],
   "source": [
    "print(f\"Were deleted but archived in SoftWH : {len(df1[df1['Correct_link'].isna()][df1['In_SoftWH'] == 1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7f6696-e4cd-4dd7-8813-87d98e8704eb",
   "metadata": {},
   "source": [
    "## Last commit analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f6e25fb5-ba02-489e-86b3-9d128619bdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_commit_files(owner, repo, access_token):\n",
    "    \n",
    "    headers = {'Authorization': f\"Token {access_token}\"}   \n",
    "    url = f'https://api.github.com/repos/{owner}/{repo}/commits'\n",
    "    response = requests.get(url, headers=headers)\n",
    "    data = response.json()\n",
    "    if response.status_code == 200:\n",
    "        if len(data) > 0:\n",
    "            last_commit_sha = data[0]['sha']\n",
    "            files_url = f'{url}/{last_commit_sha}'\n",
    "            files_response = requests.get(files_url)\n",
    "            files_data = files_response.json()\n",
    "            print(files_data)\n",
    "            if files_response.status_code == 200:\n",
    "                files_changed = [file['filename'] for file in files_data['files']]\n",
    "                return files_changed\n",
    "        \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bb6e26fe-77be-4f46-85fe-710e0cbc972f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': \"API rate limit exceeded for 46.193.56.59. (But here's the good news: Authenticated requests get a higher rate limit. Check out the documentation for more details.)\", 'documentation_url': 'https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting'}\n"
     ]
    }
   ],
   "source": [
    "owner = 'guilledufort'\n",
    "repo = 'RENANO'\n",
    "\n",
    "files_changed = get_last_commit_files(owner, repo, GITHUB_TOKEN)\n",
    "if files_changed:\n",
    "    print('Files changed in the last commit:\\n')\n",
    "    for file in files_changed:\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "456ef14d-b30b-4628-baf1-e7954b09272a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10623"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('articles.tsv', sep='\\t')\n",
    "PMIDs = df['PMID'][df['GitHub_repo'].notna()].to_list()\n",
    "len(PMIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea785a78-2c03-45d0-b432-a10ac571e80b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "565c92bb-628b-4e60-bb39-00054cd4ec60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:28<00:00,  3.57it/s]\n"
     ]
    }
   ],
   "source": [
    "for PMID in tqdm(PMIDs[:100]):\n",
    "\n",
    "    #with open(\"gitstat.txt\", \"a\") as f:\n",
    "    #    f.write(f\"\\n\\n PMID: {PMID}, GitHub link: {df[df['PMID'] == PMID]['GitHub_link_clean'].values[0]}\")\n",
    "    \n",
    "    last_commit_files = ''\n",
    "    files = get_last_commit_files(df[df['PMID']==PMID]['GitHub_owner'].values[0], df[df['PMID']==PMID]['GitHub_repo'].values[0])\n",
    "    for file in files:\n",
    "        last_commit_files += f\"{file}, \"\n",
    "        \n",
    "    idx = df.index[df['PMID'] == PMID][0]\n",
    "\n",
    "    df.loc[idx, \"Last_commit\"] = last_commit_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f989072-7dbf-45db-9ce5-3891b9daaaec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PMID</th>\n",
       "      <th>PubDate</th>\n",
       "      <th>DOI</th>\n",
       "      <th>Journal</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>GitHub_link_raw</th>\n",
       "      <th>GitHub_link_clean</th>\n",
       "      <th>GitHub_owner</th>\n",
       "      <th>GitHub_repo</th>\n",
       "      <th>Repo_created_at</th>\n",
       "      <th>Repo_updated_at</th>\n",
       "      <th>Fork</th>\n",
       "      <th>In_SoftWH</th>\n",
       "      <th>Archived</th>\n",
       "      <th>Last_commit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34337657</td>\n",
       "      <td>2021-11-05</td>\n",
       "      <td>10.1093/bib/bbab288</td>\n",
       "      <td>Briefings in bioinformatics</td>\n",
       "      <td>iDeepSubMito: identification of protein submit...</td>\n",
       "      <td>Mitochondria are membrane-bound organelles con...</td>\n",
       "      <td>github.com/houzl3416/iDeepSubMito.</td>\n",
       "      <td>https://github.com/houzl3416/iDeepSubMito/</td>\n",
       "      <td>houzl3416</td>\n",
       "      <td>iDeepSubMito</td>\n",
       "      <td>2021-04-12</td>\n",
       "      <td>2022-09-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['README.md']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>32172688</td>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>10.1186/s41065-020-00120-6</td>\n",
       "      <td>Hereditas</td>\n",
       "      <td>STRsearch: a new pipeline for targeted profili...</td>\n",
       "      <td>[OrderedDict([('@Label', 'BACKGROUND'), ('@Nlm...</td>\n",
       "      <td>github.com/AnJingwd/STRsearch.</td>\n",
       "      <td>https://github.com/AnJingwd/STRsearch/</td>\n",
       "      <td>AnJingwd</td>\n",
       "      <td>STRsearch</td>\n",
       "      <td>2019-10-12</td>\n",
       "      <td>2022-06-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-08-11</td>\n",
       "      <td>['README.md']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>35354136</td>\n",
       "      <td>2022-04-11</td>\n",
       "      <td>10.1088/1361-6560/ac628a</td>\n",
       "      <td>Physics in medicine and biology</td>\n",
       "      <td>Category guided attention network for brain tu...</td>\n",
       "      <td>. Magnetic resonance imaging (MRI) has been wi...</td>\n",
       "      <td>github.com/delugewalker/CGA-U-Net.</td>\n",
       "      <td>https://github.com/delugewalker/CGA-U-Net/</td>\n",
       "      <td>delugewalker</td>\n",
       "      <td>CGA-U-Net</td>\n",
       "      <td>2022-02-09</td>\n",
       "      <td>2022-07-20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['README.md']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>31173946</td>\n",
       "      <td>2019-05-10</td>\n",
       "      <td>10.1016/j.omtn.2019.04.025</td>\n",
       "      <td>Molecular therapy. Nucleic acids</td>\n",
       "      <td>ACP-DL: A Deep Learning Long Short-Term Memory...</td>\n",
       "      <td>Cancer is a well-known killer of human beings,...</td>\n",
       "      <td>github.com/haichengyi/ACP-DL.</td>\n",
       "      <td>https://github.com/haichengyi/ACP-DL/</td>\n",
       "      <td>haichengyi</td>\n",
       "      <td>ACP-DL</td>\n",
       "      <td>2018-08-29</td>\n",
       "      <td>2023-03-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>['README.md']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>35510918</td>\n",
       "      <td>2022-05-05</td>\n",
       "      <td>10.1021/acs.analchem.2c00500</td>\n",
       "      <td>Analytical chemistry</td>\n",
       "      <td>2 Global Tracking of Transformation Products o...</td>\n",
       "      <td>Stable isotope-assisted metabolomics (SIAM) en...</td>\n",
       "      <td>github.com/kechen1984/2H-SIAM</td>\n",
       "      <td>https://github.com/kechen1984/2H-SIAM/</td>\n",
       "      <td>kechen1984</td>\n",
       "      <td>2H-SIAM</td>\n",
       "      <td>2021-09-17</td>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['README.md']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        PMID     PubDate                           DOI   \n",
       "2   34337657  2021-11-05           10.1093/bib/bbab288  \\\n",
       "7   32172688  2020-03-16    10.1186/s41065-020-00120-6   \n",
       "8   35354136  2022-04-11      10.1088/1361-6560/ac628a   \n",
       "9   31173946  2019-05-10    10.1016/j.omtn.2019.04.025   \n",
       "17  35510918  2022-05-05  10.1021/acs.analchem.2c00500   \n",
       "\n",
       "                             Journal   \n",
       "2        Briefings in bioinformatics  \\\n",
       "7                          Hereditas   \n",
       "8    Physics in medicine and biology   \n",
       "9   Molecular therapy. Nucleic acids   \n",
       "17              Analytical chemistry   \n",
       "\n",
       "                                                Title   \n",
       "2   iDeepSubMito: identification of protein submit...  \\\n",
       "7   STRsearch: a new pipeline for targeted profili...   \n",
       "8   Category guided attention network for brain tu...   \n",
       "9   ACP-DL: A Deep Learning Long Short-Term Memory...   \n",
       "17  2 Global Tracking of Transformation Products o...   \n",
       "\n",
       "                                             Abstract   \n",
       "2   Mitochondria are membrane-bound organelles con...  \\\n",
       "7   [OrderedDict([('@Label', 'BACKGROUND'), ('@Nlm...   \n",
       "8   . Magnetic resonance imaging (MRI) has been wi...   \n",
       "9   Cancer is a well-known killer of human beings,...   \n",
       "17  Stable isotope-assisted metabolomics (SIAM) en...   \n",
       "\n",
       "                       GitHub_link_raw   \n",
       "2   github.com/houzl3416/iDeepSubMito.  \\\n",
       "7       github.com/AnJingwd/STRsearch.   \n",
       "8   github.com/delugewalker/CGA-U-Net.   \n",
       "9        github.com/haichengyi/ACP-DL.   \n",
       "17       github.com/kechen1984/2H-SIAM   \n",
       "\n",
       "                             GitHub_link_clean  GitHub_owner   GitHub_repo   \n",
       "2   https://github.com/houzl3416/iDeepSubMito/     houzl3416  iDeepSubMito  \\\n",
       "7       https://github.com/AnJingwd/STRsearch/      AnJingwd     STRsearch   \n",
       "8   https://github.com/delugewalker/CGA-U-Net/  delugewalker     CGA-U-Net   \n",
       "9        https://github.com/haichengyi/ACP-DL/    haichengyi        ACP-DL   \n",
       "17      https://github.com/kechen1984/2H-SIAM/    kechen1984       2H-SIAM   \n",
       "\n",
       "   Repo_created_at Repo_updated_at  Fork  In_SoftWH    Archived    Last_commit  \n",
       "2       2021-04-12      2022-09-02   0.0        0.0         NaN  ['README.md']  \n",
       "7       2019-10-12      2022-06-22   0.0        1.0  2020-08-11  ['README.md']  \n",
       "8       2022-02-09      2022-07-20   0.0        0.0         NaN  ['README.md']  \n",
       "9       2018-08-29      2023-03-12   0.0        1.0  2020-03-27  ['README.md']  \n",
       "17      2021-09-17      2022-03-26   0.0        0.0         NaN  ['README.md']  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Last_commit']==\"['README.md']\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
