{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43c5ca6a-125a-4118-b3d2-1e45626d1212",
   "metadata": {},
   "source": [
    "# Current Trends in Bioinformatics Software Development and Archiving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2092ef92-81df-4128-a1fa-f728c558bac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "\n",
    "import dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import warnings\n",
    "import xmltodict\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import pbmd_tools as pbmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e66c109-89bd-4f6f-8d99-89ed18a4be3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbmd.read_tokens()\n",
    "GITHUB_TOKEN = os.environ.get(\"GITHUB_TOKEN\")\n",
    "PUBMED_TOKEN = os.environ.get(\"PUBMED_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0068b37-9ae4-444b-874d-2cd5e173b7e5",
   "metadata": {},
   "source": [
    "## 1. PubMed API Entrez Interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81134853-e4da-4e60-aff1-f34443f4ae7b",
   "metadata": {},
   "source": [
    "First of all we are going to explore PubMed in order to find out how many publications for each among 5 forges are there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "188978bd-1f8f-4fc5-9928-9157a341cda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = \"pubmed\"\n",
    "domain = \"https://www.ncbi.nlm.nih.gov/entrez/eutils\"\n",
    "retmode = \"json\"\n",
    "queries_github = []\n",
    "queries_gitlab = []\n",
    "queries_sourceforge = []\n",
    "queries_googlecode = []\n",
    "queries_bitbucket = []\n",
    "\n",
    "#creating queries for every forge and every year\n",
    "for year in range(2009, 2023):\n",
    "    queries_github.append(f'((github.com[Title/Abstract])) AND ((\"{year}/01/01\"[Date - Publication] : \"{year}/12/31\"[Date - Publication]))')\n",
    "    queries_gitlab.append(f'((https://gitlab[Title/Abstract])) AND ((\"{year}/01/01\"[Date - Publication] : \"{year}/12/31\"[Date - Publication]))')\n",
    "    queries_sourceforge.append(f'((sourceforge.net[Title/Abstract])) AND ((\"{year}/01/01\"[Date - Publication] : \"{year}/12/31\"[Date - Publication]))')\n",
    "    queries_googlecode.append(f'(googlecode) AND (\"{year}/01/01\"[Date - Publication] : \"{year}/12/31\"[Date - Publication])')\n",
    "    queries_bitbucket.append(f'(bitbucket.org[Title/Abstract]) AND (\"{year}/01/01\"[Date - Publication] : \"{year}/12/31\"[Date - Publication])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "975bb2c5-a417-48a3-adcd-198c2cb08332",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 14/14 [00:11<00:00,  1.22it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 14/14 [00:09<00:00,  1.46it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 14/14 [00:08<00:00,  1.66it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 14/14 [00:08<00:00,  1.59it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 14/14 [00:09<00:00,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10882 articles with 'github.com' found in PubMed\n",
      "\n",
      "10882 unique articles with 'github.com' found in PubMed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#dictionaries for stocking the number of articles for each forge for each year\n",
    "#example: {'2009': 0, '2010': 5, '2011': 15, ... }\n",
    "\n",
    "stats_github = {}\n",
    "stats_gitlab = {}\n",
    "stats_sourceforge = {}\n",
    "stats_googlecode = {}\n",
    "stats_bitbucket = {}\n",
    "PMIDs = []\n",
    "PMIDs_all = []\n",
    "\n",
    "for query in tqdm(queries_github):\n",
    "    nb = 0 #number of articles for this query\n",
    "    queryLinkSearch = f\"{domain}/esearch.fcgi?db={db}&retmode={retmode}&retmax=15000&term={query}\"\n",
    "    response = requests.get(queryLinkSearch)\n",
    "    pubmed_json = response.json()\n",
    "    for id in pubmed_json[\"esearchresult\"][\"idlist\"]:\n",
    "        #checking if there are any dublicates in PubMed IDs (it happens because of the PubDate that can be EPubDate or normal)\n",
    "        if id not in PMIDs:\n",
    "            nb += 1\n",
    "            PMIDs.append(id)\n",
    "    #query[38:42] - it is the year of this query\n",
    "    stats_github[query[38:42]] = nb \n",
    "    \n",
    "for query in tqdm(queries_bitbucket):\n",
    "    nb = 0\n",
    "    queryLinkSearch = f\"{domain}/esearch.fcgi?db={db}&retmode={retmode}&retmax=15000&term={query}\"\n",
    "    response = requests.get(queryLinkSearch)\n",
    "    pubmed_json = response.json()\n",
    "    for id in pubmed_json[\"esearchresult\"][\"idlist\"]:\n",
    "        if id not in PMIDs_all:\n",
    "            nb += 1\n",
    "            PMIDs_all.append(id)\n",
    "    stats_bitbucket[query[38:42]] = nb\n",
    "    \n",
    "for query in tqdm(queries_gitlab):\n",
    "    nb = 0\n",
    "    queryLinkSearch = f\"{domain}/esearch.fcgi?db={db}&retmode={retmode}&retmax=15000&term={query}\"\n",
    "    response = requests.get(queryLinkSearch)\n",
    "    pubmed_json = response.json()\n",
    "    for id in pubmed_json[\"esearchresult\"][\"idlist\"]:\n",
    "        if id not in PMIDs_all:\n",
    "            nb += 1\n",
    "            PMIDs_all.append(id)\n",
    "    stats_gitlab[query[42:46]] = nb\n",
    "    \n",
    "for query in tqdm(queries_sourceforge):\n",
    "    nb = 0\n",
    "    queryLinkSearch = f\"{domain}/esearch.fcgi?db={db}&retmode={retmode}&retmax=15000&term={query}\"\n",
    "    response = requests.get(queryLinkSearch)\n",
    "    pubmed_json = response.json()\n",
    "    for id in pubmed_json[\"esearchresult\"][\"idlist\"]:\n",
    "        if id not in PMIDs_all:\n",
    "            nb += 1\n",
    "            PMIDs_all.append(id)\n",
    "    stats_sourceforge[query[43:47]] = nb\n",
    "    \n",
    "for query in tqdm(queries_googlecode):\n",
    "    nb = 0\n",
    "    queryLinkSearch = f\"{domain}/esearch.fcgi?db={db}&retmode={retmode}&retmax=15000&term={query}\"\n",
    "    response = requests.get(queryLinkSearch)\n",
    "    pubmed_json = response.json()\n",
    "    for id in pubmed_json[\"esearchresult\"][\"idlist\"]:\n",
    "        if id not in PMIDs_all:\n",
    "            nb += 1\n",
    "            PMIDs_all.append(id)\n",
    "    stats_googlecode[query[19:23]] = nb\n",
    "\n",
    "print(f\"\\n{len(PMIDs)} articles with 'github.com' found in PubMed\")\n",
    "\n",
    "#checking that there is no duplicates\n",
    "PMIDs = list(set(PMIDs))\n",
    "print(f\"\\n{len(PMIDs)} unique articles with 'github.com' found in PubMed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e968d76d-0ce9-460f-80c4-1803faebded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the statistics to reuse it in another notebook\n",
    "\n",
    "with open(\"PMIDs.txt\", \"w\") as f:\n",
    "    for PMID in PMIDs:\n",
    "        f.write(str(PMID)+\"\\n\")\n",
    "with open(\"stats_github.json\", \"w\") as f:\n",
    "    json.dump(stats_github, f)\n",
    "with open(\"stats_gitlab.json\", \"w\") as f:\n",
    "    json.dump(stats_gitlab, f)\n",
    "with open(\"stats_sourceforge.json\", \"w\") as f:\n",
    "    json.dump(stats_sourceforge, f)    \n",
    "with open(\"stats_googlecode.json\", \"w\") as f:\n",
    "    json.dump(stats_googlecode, f)\n",
    "with open(\"stats_bitbucket.json\", \"w\") as f:\n",
    "    json.dump(stats_bitbucket, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "98176cc7-615e-4060-84e2-f2b2dd617b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pubmed.ncbi.nlm.nih.gov/26262258/ - No DOI in PubMed although there is one in the full text of the article (not from ArXiv), and there are a lot of them\n",
    "# https://pubmed.ncbi.nlm.nih.gov/28269829/ - they give a wrong link https://github.com/SBU-BMI/imageboxs://github.com/SBU-BMI/imagebox but if you use this link :\n",
    "# https://github.com/SBU-BMI/imagebox it works. Yet, i am not sure that it is actually what we are looking for since they also provide another link to github.io \n",
    "# (also incorect) and i think it's more likely that their code is there\n",
    "# PMID = 36789260 - 2 links\n",
    "# https://github.com/tyqGitHub/TYQ/tree/master/GACNNMDA - ????\n",
    "# https://github.com/mofradlab - ?????? (PMID 36786404)\n",
    "# PMID = 26124555 - a space in the link\n",
    "# PMID = 24324759, 22151646 - no space after link\n",
    "# PMID = 23849037 - why + in the end ?\n",
    "# PMID = 36315552 - super smart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4aac81-c8b6-4712-98c2-f0a3e9ff1563",
   "metadata": {},
   "source": [
    "Next we will use API PubMed to gather the information about each article such as the publication date, the doi, the abstract, the title of the article and the journal. We will then analyse this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e8425aae-1da2-443d-a325-dc4a1fbe4814",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b5b5aead-fa36-41c6-af77-8af8138b0ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [01:40<00:00,  1.70s/it]\n"
     ]
    }
   ],
   "source": [
    "#API Pubmed rate limit is 10 request per second with a token and 3 request par second without it\n",
    "\n",
    "#count = 0\n",
    "for PMID in tqdm(pm):\n",
    "    #count += 1\n",
    "    #if count % 10 == 0:\n",
    "    #    time.sleep(1)\n",
    "    \n",
    "    try:\n",
    "        summary = pbmd.get_summary(PMID, PUBMED_TOKEN, \"status.txt\")\n",
    "        abstract = pbmd.get_abstract_from_summary(summary, \"status.txt\")\n",
    "        pubdate = pbmd.get_pubdate_from_summary(summary, \"status.txt\")\n",
    "        title = pbmd.get_title_from_summary(summary, \"status.txt\")\n",
    "        journal = pbmd.get_journal_from_summary(summary, \"status.txt\")\n",
    "        doi = pbmd.get_doi_from_summary(summary, \"status.txt\")\n",
    "    except:\n",
    "        try:\n",
    "            summary = pbmd.get_summary(PMID, PUBMED_TOKEN, \"status.txt\")\n",
    "            abstract = pbmd.get_abstract_from_summary(summary, \"status.txt\")\n",
    "            pubdate = pbmd.get_pubdate_from_summary(summary, \"status.txt\")\n",
    "            title = pbmd.get_title_from_summary(summary, \"status.txt\")\n",
    "            journal = pbmd.get_journal_from_summary(summary, \"status.txt\")\n",
    "            doi = pbmd.get_doi_from_summary(summary, \"status.txt\")\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "    #checking in case the API is bugging \n",
    "    if (pubdate, doi) == (None, None):\n",
    "        time.sleep(2)\n",
    "        summary = pbmd.get_summary(PMID, PUBMED_TOKEN, \"status.txt\")\n",
    "        abstract = pbmd.get_abstract_from_summary(summary, \"status.txt\")\n",
    "        pubdate = pbmd.get_pubdate_from_summary(summary, \"status.txt\")\n",
    "        title = pbmd.get_title_from_summary(summary, \"status.txt\")\n",
    "        journal = pbmd.get_journal_from_summary(summary, \"status.txt\")\n",
    "        doi = pbmd.get_doi_from_summary(summary, \"status.txt\")     \n",
    "\n",
    "    results.append((PMID, pubdate, doi, journal, title, abstract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e11bc34d-7377-4167-8d80-48952e78323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(results)\n",
    "df = df.rename(columns = {0: 'PMID', 1: 'PubDate', 3: 'DOI', 4: 'Journal', 5: 'Title', 6: 'Abstract'})\n",
    "df = df.drop_duplicates(subset = 'PMID')\n",
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8a3bcdb5-1810-4b53-b6e9-f55e7b4c3d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10880"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9a6c9a6e-9a56-4117-b4c9-28bec1d25313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records without publication date is: 59\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of records without publication date is: {len(df[df['PubDate'].isnull()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bbf1270-ff48-48bb-b910-dcdbca765190",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df.columns[-2:], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94f8544e-f411-40c6-bd35-318d762992e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('articles.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d566e858-dc91-44d1-9e85-13a0fc89f892",
   "metadata": {},
   "source": [
    "## 2. Geting links from the obtained data using regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5f1cfe1c-42eb-4958-a13c-8c7d43136ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('articles.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b8ce85de-20f7-4211-830b-4c6e01e0eb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GitHub_link_raw'] = df['Abstract'].astype(str).apply(pbmd.get_link_from_abstract)\n",
    "df['GitHub_link_clean'] = df['GitHub_link_raw'].astype(str).apply(pbmd.clean_link)\n",
    "df['GitHub_owner'] = df['GitHub_link_clean'].apply(pbmd.get_owner_from_link)\n",
    "df['GitHub_repo'] = df['GitHub_link_clean'].apply(pbmd.get_repo_from_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "964e174c-558c-445e-abec-a0b0c2c16b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records with weird abstracts leading to inability to extract a link: 6\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of records with weird abstracts leading to inability to extract a link: {len(df[df['GitHub_owner'].isna()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1d2ae2ce-d877-42af-84d7-96db53d9dae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records without a repository name: 251\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of records without a repository name: {len(df[df['GitHub_repo'].isna()])-len(df[df['GitHub_owner'].isna()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "84639824-a280-494b-b8e8-d2a5b6981006",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('articles.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d124283b-c8d7-4c6a-913f-f64f2390d66a",
   "metadata": {},
   "source": [
    "## 3. GitHub API Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "160e77c7-4980-4dbc-bc92-ff9319cd9480",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('articles.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f6bfbf59-53a2-4a9e-93cd-f509588406a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10623"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PMIDs = df['PMID'][df['GitHub_repo'].notna()].to_list()\n",
    "len(PMIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "36dd1992-56f7-42a8-95ae-810217f93e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 905/905 [05:39<00:00,  2.67it/s]\n"
     ]
    }
   ],
   "source": [
    "for PMID in tqdm(PMIDs[9718:]):\n",
    "\n",
    "    with open(\"gitstat.txt\", \"a\") as f:\n",
    "        f.write(f\"\\n\\n PMID: {PMID}, GitHub link: {df[df['PMID'] == PMID]['GitHub_link_clean'].values[0]}\")\n",
    "\n",
    "    info = pbmd.get_repo_info(df[df['PMID']==PMID]['GitHub_owner'].values[0], df[df['PMID']==PMID]['GitHub_repo'].values[0], GITHUB_TOKEN, \"gitstat.txt\")\n",
    "\n",
    "    if info[\"status\"]: \n",
    "        idx = df.index[df['PMID'] == PMID][0]\n",
    "\n",
    "        df.loc[idx, \"Repo_created_at\"] = pbmd.get_repo_date_created(info)\n",
    "        df.loc[idx, \"Repo_updated_at\"] = pbmd.get_repo_date_updated(info)\n",
    "        df.loc[idx, \"Fork\"] = pbmd.is_fork(info)\n",
    "    else:\n",
    "        \n",
    "        time.sleep(3600)\n",
    "        \n",
    "        info = pbmd.get_repo_info(df[df['PMID']==PMID]['GitHub_owner'].values[0], df[df['PMID']==PMID]['GitHub_repo'].values[0], GITHUB_TOKEN, \"gitstat.txt\")\n",
    "        idx = df.index[df['PMID'] == PMID][0]\n",
    "        df.loc[idx, \"Repo_created_at\"] = pbmd.get_repo_date_created(info)\n",
    "        df.loc[idx, \"Repo_updated_at\"] = pbmd.get_repo_date_updated(info)\n",
    "        df.loc[idx, \"Fork\"] = pbmd.is_fork(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682e0960-eefb-4676-a049-e22cca92418f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "758"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PMIDs = df['PMID'][df['Repo_created_at'].isna()].to_list()\n",
    "len(PMIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fe532216-f508-4ff9-a16e-3bb957b1e344",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('articles.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad760f81-0cf8-4b5d-9015-bda6ac2d2487",
   "metadata": {},
   "source": [
    "## 4. Software Heritage API interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aa51fd7-bd8c-472f-b659-edb3ded49461",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('articles.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca4481bb-3767-415c-bc0d-d460cdc7c5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10847"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PMIDs = df['PMID'][df['GitHub_owner'].notna()].to_list()\n",
    "len(PMIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d60a2e7-1d01-4b23-8500-221e4a29c748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10847/10847 [19:47<00:00,  9.13it/s]  \n"
     ]
    }
   ],
   "source": [
    "for PMID in tqdm(PMIDs[:]):\n",
    "    \n",
    "    try:\n",
    "        info = pbmd.check_is_in_softwh(df[df['PMID']==PMID]['GitHub_link_clean'].values[0])\n",
    "\n",
    "        idx = df.index[df['PMID'] == PMID][0]\n",
    "\n",
    "        df.loc[idx, \"In_SoftWH\"] = pbmd.is_in_softwh(info)\n",
    "        df.loc[idx, \"Archived\"] = pbmd.get_date_archived(info)\n",
    "    except:\n",
    "        try:\n",
    "            info = pbmd.check_is_in_softwh(df[df['PMID']==PMID]['GitHub_link_clean'].values[0])\n",
    "\n",
    "            idx = df.index[df['PMID'] == PMID][0]\n",
    "\n",
    "            df.loc[idx, \"In_SoftWH\"] = pbmd.is_in_softwh(info)\n",
    "            df.loc[idx, \"Archived\"] = pbmd.get_date_archived(info)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3954853c-f227-4a5d-8c24-99694669cdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('articles.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d478c7b-9a74-42fb-b71d-1c594eefdadc",
   "metadata": {},
   "source": [
    "## Unresolved links analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b597f0b6-78f4-4736-9a89-ac0191b0c702",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('no_info2.tsv', sep='\\t',usecols=['PMID', 'PubDate', 'DOI', 'Journal', 'Title', 'Abstract', 'Issue', 'GitHub_link_clean','Correct_link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "6b70ec1b-96dd-43d1-9876-4dac90c542a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles with a wrong link (either a space in the link, or no space after link, etc) : 58\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of articles with a wrong link (either a space in the link, or no space after link, etc) : {len(df1[df1['Issue'] == 'wrong link'])}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "b34f0e2f-1e42-46ff-ae71-f984e2a433a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles with a renamed repository : 11\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of articles with a renamed repository : {len(df1[df1['Issue'] == 'renamed'])}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "a8db6bd5-d473-4a5f-85ab-e8b179186b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles with a deleted repository : 146\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of articles with a deleted repository : {len(df1[df1['Issue'] == 'owner deleted']) + len(df1[df1['Issue'] == 'repo deleted'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fd77b8-8cac-45be-af2f-928dd2d35cc5",
   "metadata": {},
   "source": [
    "Resolving unresolved but existing links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "3db1fcb0-8066-4c19-a853-34ad54833439",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.loc[df['Correct_link'].notna(),'GitHub_owner'] = df1.loc[df['Correct_link'].notna(),'Correct_link'].apply(pbmd.get_owner_from_link)\n",
    "df1.loc[df['Correct_link'].notna(),'GitHub_repo'] = df1.loc[df['Correct_link'].notna(),'Correct_link'].apply(pbmd.get_repo_from_link)\n",
    "\n",
    "df1.loc[df['Correct_link'].isna(),'GitHub_owner'] = df1.loc[df['Correct_link'].isna(),'GitHub_link_clean'].apply(pbmd.get_owner_from_link)\n",
    "df1.loc[df['Correct_link'].isna(),'GitHub_repo'] = df1.loc[df['Correct_link'].isna(),'GitHub_link_clean'].apply(pbmd.get_repo_from_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "35967c10-c389-4542-9dfb-cca1c8a61306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PMIDs = df1['PMID'][df1['GitHub_repo'].notna()].to_list()\n",
    "len(PMIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "2e7d124d-9a2b-443b-8c63-7ee3cbecc7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 215/215 [01:05<00:00,  3.30it/s]\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for PMID in tqdm(PMIDs):\n",
    "    count += 1\n",
    "    if count % 5000 == 0:\n",
    "        time.sleep(3600)\n",
    "    \n",
    "    with open(\"gitstat.txt\", \"a\") as f:\n",
    "        f.write(f\"\\n\\n PMID: {PMID}, GitHub link: {df1[df1['PMID'] == PMID]['Correct_link'].values[0]}\")\n",
    "\n",
    "    info = pbmd.get_repo_info(df1[df1['PMID']==PMID]['GitHub_owner'].values[0], df1[df1['PMID']==PMID]['GitHub_repo'].values[0], GITHUB_TOKEN, \"gitstat.txt\")\n",
    "\n",
    "    idx = df1.index[df['PMID'] == PMID][0]\n",
    "\n",
    "    df1.loc[idx, \"Repo_created_at\"] = pbmd.get_repo_date_created(info)\n",
    "    df1.loc[idx, \"Repo_updated_at\"] = pbmd.get_repo_date_updated(info)\n",
    "    df1.loc[idx, \"Fork\"] = pbmd.is_fork(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "ae1722cd-c089-456a-97b4-5267d944a463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PMIDs = df1['PMID'][df1['Correct_link'].notna()].to_list()\n",
    "len(PMIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "3ea729a9-2a5a-4d81-99c8-7e2718000724",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 69/69 [00:13<00:00,  5.03it/s]\n"
     ]
    }
   ],
   "source": [
    "for PMID in tqdm(PMIDs):\n",
    "    \n",
    "    info = pbmd.check_is_in_softwh(df1[df1['PMID']==PMID]['Correct_link'].values[0])\n",
    "\n",
    "    idx = df1.index[df1['PMID'] == PMID][0]\n",
    "    \n",
    "    df1.loc[idx, \"In_SoftWH\"] = pbmd.is_in_softwh(info)\n",
    "    df1.loc[idx, \"Archived\"] = pbmd.get_date_archived(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "d3c3010b-1b18-4875-bf57-c3e100883ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PMIDs = df1['PMID'][df1['Correct_link'].isna()].to_list()\n",
    "len(PMIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "0d615f28-b18d-4cd0-9407-5d6e88b9e7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 146/146 [00:56<00:00,  2.60it/s]\n"
     ]
    }
   ],
   "source": [
    "for PMID in tqdm(PMIDs):\n",
    "    \n",
    "    info = pbmd.check_is_in_softwh(df1[df1['PMID']==PMID]['GitHub_link_clean'].values[0])\n",
    "\n",
    "    idx = df1.index[df1['PMID'] == PMID][0]\n",
    "    \n",
    "    df1.loc[idx, \"In_SoftWH\"] = pbmd.is_in_softwh(info)\n",
    "    df1.loc[idx, \"Archived\"] = pbmd.get_date_archived(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "414b497b-bf2c-432e-86c7-cfe37b3af1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repositories that are forks : 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Repositories that are forks : {len(df1[df1['Fork'] == 1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "2433e02b-40b0-4b61-8636-a35d3954f887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in SoftWH : 118\n"
     ]
    }
   ],
   "source": [
    "print(f\"Not in SoftWH : {len(df1[df1['In_SoftWH'] == 0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "b561b266-3336-4238-8827-87c3bef2740d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In SoftWH : 97\n"
     ]
    }
   ],
   "source": [
    "print(f\"In SoftWH : {len(df1[df1['In_SoftWH'] == 1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "23fad432-e0f1-4cb4-991b-a88ca809422d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Were deleted but archived in SoftWH : 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nadia\\AppData\\Local\\Temp\\ipykernel_9668\\3996960915.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  print(f\"Were deleted but archived in SoftWH : {len(df[df['Correct_link'].isna()][df['In_SoftWH'] == 1])}\")\n"
     ]
    }
   ],
   "source": [
    "print(f\"Were deleted but archived in SoftWH : {len(df1[df1['Correct_link'].isna()][df1['In_SoftWH'] == 1])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
